---
title: "Machine Learning I"
subtitle: "Text Analytics"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,warning = FALSE)
```


# Librerías 
```{r}
library(tidyverse) # Manipulación de datos 
library(tidymodels) # Machine Learning 
library(ggrepel) # Gráficos
library(irlba) # Examinar patrones 
library(tidytext) # Manipulación de análisis textual
library(tidylo) # Análisis Bayesiano de los datos 
library(textrecipes) # Construir modelos
library(keras) # Librería de deep learning
```


## Bases de datos 

```{r}
key_crop_yields <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-01/key_crop_yields.csv")
land_use <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-01/land_use_vs_yield_change_in_cereal_production.csv")

```

## Short EDA

UN poco de limpieza
```{r}
top_countries <- land_use %>%
  janitor::clean_names() %>% # Con esta función se limpian los nombres
  filter(!is.na(code), entity != "World") %>% # Se eliminan los valores vacios 
  group_by(entity) %>%
  filter(year == max(year)) %>% # Se selecciona el máximo de los años por país
  ungroup() %>%
  slice_max(total_population_gapminder, n = 30) %>%
  pull(entity)

top_countries
```

Ahora se ordenan los datos a través de su producción

```{r}
tidy_yields <- key_crop_yields %>%
  janitor::clean_names() %>%
  pivot_longer(wheat_tonnes_per_hectare:bananas_tonnes_per_hectare,
    names_to = "crop", values_to = "yield"
  ) %>%
  mutate(crop = str_remove(crop, "_tonnes_per_hectare")) %>%
  filter(
    crop %in% c("wheat", "rice", "maize", "barley"),
    entity %in% top_countries,
    !is.na(yield)
  )

tidy_yields
```

¿Cómo leemos está gráfica?

```{r, fig.height=4}
tidy_yields %>%
  ggplot(aes(year, yield, color = crop)) +
  geom_line(alpha = 0.7, size = 1.5) +
  geom_point() +
  facet_wrap(~entity, ncol = 5) +
  scale_x_continuous(guide = guide_axis(angle = 90)) +
  labs(x = NULL, y = "Producción por toneladas")
```

## Construir modelos lineales 

```{r}
tidy_lm <- tidy_yields %>%
  nest(yields = c(year, yield)) %>%
  mutate(model = map(yields, ~ lm(yield ~ year, data = .x)))

slopes <- tidy_lm %>%
  mutate(coefs = map(model, tidy)) %>%
  unnest(coefs) %>%
  filter(term == "year") %>%
  mutate(p.value = p.adjust(p.value))

slopes %>%
  ggplot(aes(estimate, p.value, label = entity)) +
  geom_vline(
    xintercept = 0, lty = 2,
    size = 1.5, alpha = 0.7, color = "gray50"
  ) +
  geom_point(aes(color = crop), alpha = 0.8, size = 2.5, show.legend = FALSE) +
  scale_y_log10() +
  facet_wrap(~crop) +
  geom_text_repel(size = 3) +
  theme(strip.text = element_text( size = 12)) +
  labs(x = "Incremento en toneladas por año")
```
## NLP y Machine Learning

```{r}

user_reviews <- readr::read_tsv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/user_reviews.tsv')
```

```{r}
user_reviews %>% 
  count(grade) %>% 
  ggplot(aes(x = grade, y = n)) + geom_col()
```

```{r}
## Creamos los corpus y matrices de palabras
sparse_df <- user_reviews %>% 
  select(grade, text) %>% 
  unnest_tokens("word", "text") %>% 
  count(grade, word) %>% 
  anti_join(stop_words) %>% 
  filter(n >= 5) %>% 
  group_by(grade) %>% 
  top_n(n, n = 25) %>% 
  ungroup() %>% 
  cast_sparse(row = grade, column = word, value = n)

# Desarrollamos los PCAs
pca_text <- prcomp_irlba(sparse_df, n = 4, scale. = TRUE)

pca_text$center %>% 
  tidy() %>% 
  select(names) %>% 
  cbind(pca_text$rotation) %>% 
  ggplot(aes(x = PC1, y = PC2, label = names)) + 
  geom_point() + 
  geom_text()
```

```{r}

user_reviews %>% 
  unnest_tokens("word", "text") %>% 
  count(grade, word) %>% 
  anti_join(stop_words) %>% 
  filter(n >= 5) %>% 
  bind_tf_idf(word, grade, n) %>% 
  group_by(grade) %>% 
  top_n(tf_idf, n =5) %>% 
  ungroup() %>% 
  mutate(grade = as.factor(grade)) %>% 
  ggplot(aes(x = reorder_within(word, tf_idf, grade), y = tf_idf, fill = grade)) + 
  geom_col() + 
  scale_x_reordered() + 
  coord_flip() + 
  facet_wrap(~grade, scales = "free") + 
  theme(legend.position = 'none')
```




```{r}
user_reviews %>% 
  unnest_tokens("word", "text") %>% 
  count(grade, word) %>% 
  anti_join(stop_words) %>% 
  filter(n >= 5) %>% 
  bind_log_odds(grade, word, n) %>% 
  mutate(grade=as.character(grade))%>%
  group_by(grade) %>% 
  top_n(log_odds_weighted, n =5) %>% 
  ungroup() %>% 
  mutate(grade = as.factor(grade)) %>% 
  ggplot(aes(x = reorder_within(word, log_odds_weighted, grade), y = log_odds_weighted, fill = grade)) + 
  geom_col() + 
  scale_x_reordered() + 
  coord_flip() + 
  facet_wrap(~grade, scales = "free") + 
  theme(legend.position = 'none')
```
## Creando un modelo de Machine Learning
```{r}

set.seed(42)
tidy_data <- user_reviews %>% select(-user_name)
tidy_split <- initial_split(tidy_data, p = .8)
tidy_train <- training(tidy_split)
tidy_test <- testing(tidy_split)

```


```{r}

text_recipe <- recipe(grade~text, data = tidy_train) %>% 
  step_tokenize(text) %>% 
  step_stopwords(text) %>% 
  step_tokenfilter(text, max_tokens = 500) %>% 
  step_tf(text)
text_prep <- text_recipe %>% prep()
cross_validation <- vfold_cv(tidy_train, v = 10)
wf <- workflow() %>% 
  add_recipe(text_recipe)
```

```{r}
lasso_model <- linear_reg(penalty = tune(), mixture = 1) %>% 
  set_mode("regression") %>% 
  set_engine("glmnet")
lasso_grid <- grid_regular(penalty(), levels = 10)
lasso_tune <- tune_grid(
  wf %>% add_model(lasso_model),
  resamples = cross_validation,
  grid = lasso_grid
)
lasso_tune %>% 
  collect_metrics() %>% 
  ggplot(aes(x = penalty, y = mean)) + geom_line() + facet_wrap(~.metric, scales = "free")
lasso_best_tune <- lasso_tune %>% select_best("rmse")
  
final_lasso_model <- finalize_model(lasso_model, lasso_best_tune)  
lasso_wf <- workflow() %>% 
  add_recipe(text_recipe) %>% 
  add_model(final_lasso_model)
lasso_eval <- lasso_wf %>% last_fit(tidy_split)
lasso_eval %>% collect_metrics()
```


```{r}
random_forest_model <- rand_forest(mtry = 25,
                                   trees = 1000,
                                   min_n = 20) %>% 
  set_mode("regression") %>% 
  set_engine("randomForest")
random_forest_tune <- fit_resamples(
  random_forest_model,
  text_recipe,
  cross_validation
)
random_forest_tune %>% 
  collect_metrics()
final_rf_wf <- workflow() %>% 
  add_recipe(text_recipe) %>% 
  add_model(random_forest_model) 
final_rf_eval <- final_rf_wf %>% last_fit(tidy_split)
```

```{r}

final_rf_eval %>% 
  collect_metrics() %>% 
  mutate(model = "rf") %>% 
  rbind(lasso_eval %>% 
          collect_metrics() %>% 
          mutate(model = "lasso")) %>% 
  ggplot(aes(x = model, y = .estimate, fill = model)) +
  geom_col() + 
  facet_wrap(~.metric, scales = "free")
```


```{r}

keras_df <- user_reviews %>% 
  select(-user_name)
max_features <- 2000
tokenizer <- text_tokenizer(num_words = max_features) %>% 
  fit_text_tokenizer(keras_df$text)
one_hot_results <- texts_to_matrix(tokenizer = tokenizer, keras_df$text, mode = "binary")
word_index <- tokenizer$word_index
```

```{r}

x_train <- one_hot_results[tidy_split$in_id,]
x_test <- one_hot_results[-tidy_split$in_id,]
y_train <- keras_df$grade[tidy_split$in_id]
y_test <- keras_df$grade[-tidy_split$in_id]
maxlen <- 128
x_train <- pad_sequences(x_train, maxlen = maxlen)
x_test <- pad_sequences(x_test, maxlen = maxlen)
```


```{r}
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_features,
                  output_dim = 16,
                  input_length = maxlen) %>% 
  layer_flatten() %>% 
  layer_dense(units = 1)
model %>% compile(
  optimizer = "rmsprop",
  loss = "mse",
  metrics = c("mae")
)
 
history <- model %>% 
  fit(
    x_train,
    y_train,
    validation_split = .2,
    epochs = 100,
    batch_size = 64
  )
```



```{r}
plot(history)
```


```{r}
evaluate(model,
         x_test,
         y_test)
```











