---
title: "Topic modeling"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
rm(list = ls())
library(tidyverse)
library(tidytext)
library(GGally)
library(MASS)
library(topicmodels)
library(stopwords)
library(widyr)
library(stm)
library(tm)
theme_set(theme_classic())
```

## Cargar base de datos 

```{r}
boton <-read.csv('../Bases_de_datos/ss.txt',header = F,sep = ';')
rojo <- read.csv('../Bases_de_datos/jb.txt',header = F,sep = ';')

boton<-boton%>%
  tbl_df()%>%
  rename(Letra=V1)%>%
  mutate(line=row_number())%>%
  mutate(cantante='Systema Solar')

rojo<-rojo%>%
  tbl_df()%>%
  rename(Letra=V1)%>%
  mutate(line=row_number())%>%
  mutate(cantante='J Balvin')
```




## Se crea una matriz 


```{r}
library(tidytext)
canciones<-bind_rows(boton,rojo)
canciones_palabras<-canciones%>%
  unnest_tokens(word,Letra)%>%
  count(cantante,word,sort = TRUE)%>%
  filter(!word %in% stopwords(kind = 'spanish'),
         !word %in% c('oh'))

canciones_palabras
```

```{r}
total_words <- canciones_palabras %>%
  group_by(cantante) %>%
  summarize(total = sum(n))

total_words
```



```{r}
canciones_words <- left_join(canciones_palabras, total_words)
canciones_words
```


Evaluemos la distribución del uso de palabras por cantante

```{r}
canciones_words%>%
  ggplot(aes(n/total, fill=cantante))+
  geom_histogram(show.legend = FALSE,bins = 8)+
  facet_wrap(~cantante)
```

Ahora el clásico tf_idf

```{r}
songs_tf_idf <- canciones_words %>%
  bind_tf_idf(word, cantante, n)

songs_tf_idf
```

```{r}
library(tidyverse)
songs_tf_idf%>%
  arrange(desc(tf_idf))
```



```{r}
songs_tf_idf %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>%
  group_by(cantante) %>%
  top_n(15) %>%
  ungroup() %>%
  ggplot(aes(word, tf_idf, fill = cantante)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~cantante, ncol = 2, scales = "free") +
  coord_flip()
```




Y un poco de bigram


```{r}
canciones_bigrams <- bind_rows(boton,rojo) %>%
  unnest_tokens(bigram, Letra, token = "ngrams", n = 3)%>%
  filter(!is.na(bigram))

canciones_bigrams
```




```{r}
canciones_bigrams %>%
  count(bigram, sort = TRUE)
```


```{r}
bigrams_separated <- canciones_bigrams %>%
  separate(bigram, c("word1", "word2","word3"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stopwords(kind  = 'spanish')) %>%
  filter(!word2 %in% stopwords(kind = 'spanish')) %>%
  filter(!word3 %in% stopwords(kind = 'spanish'))


bigram_counts <- bigrams_filtered %>%
  count(word1, word2,word3, sort = TRUE)

bigram_counts
```

```{r}
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2,word3, sep = " ")
bigrams_united
```

Ahora el tf-idf del bigram

```{r}
bigram_tf_idf <- bigrams_united %>%
  count(cantante, bigram) %>%
  bind_tf_idf(bigram, cantante, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf
```


```{r}
bigram_tf_idf%>%
  mutate(cantante=as.factor(cantante),
         bigram=reorder_within(bigram,tf_idf,cantante))%>%
  ggplot(aes(tf_idf,bigram,fill=cantante))+
  geom_col(show.legend = FALSE)+
  facet_wrap(~cantante, scales = 'free')+
  scale_y_reordered()
```


```{r}
canciones_dtm<-bind_rows(boton,rojo)%>%
  unnest_tokens(word, Letra) %>%
  count(cantante, word) %>%
  cast_dtm(cantante, word, n)
```


Ahora si el LDA
```{r}
canciones_lda<-LDA(canciones_dtm,k = 2,control = list(seed=1234))
canciones_lda
```

```{r}
canciones_topics <- tidy(canciones_lda, matrix = "beta")
canciones_topics<-canciones_topics%>%
  filter(!term %in% stopwords(kind = 'spanish'))
```


```{r}
ap_top_terms <- canciones_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)


ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic, scales = "free") +
  coord_flip() +
  scale_x_reordered()
```



```{r}
beta_spread <- canciones_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

beta_spread%>%
  mutate(term=fct_reorder(term,log_ratio))%>%
  head(30)%>%
  ggplot(aes(log_ratio,term,fill=log_ratio>0))+
  geom_col()
```


Para evitar la confusión de arriba 


```{r}
canciones_documents <- tidy(canciones_lda, matrix = "gamma")
canciones_documents%>%
  group_by(topic)%>%
  top_n(1)
  
  
```



```{r}
canciones_topics <- tidy(canciones_lda, matrix = "beta")
terms(canciones_lda,threshold=.0000000009)
```


## NER

```{r}
canciones_lda<-LDA(canciones_dtm,k = 2,method = 'Gibbs',control = list(seed=1234,iter=700, thin=1))
```

```{r}
tidy(canciones_lda,"gamma")%>%
  spread(topic,gamma)
```


```{r, fig.height=4}
patron<-"[A-Z][a-z]+"
boton_pull<-boton%>%
  pull(Letra)%>%
  paste(collapse = " ")

boton_pull

```

```{r}
m <- gregexpr(patron, boton_pull)
m%>%unlist()
v <- unlist(regmatches(boton_pull, m))
v
```

```{r}
Rojo_pull<-rojo%>%
  pull(Letra)%>%
  paste(collapse = " ")

m <- gregexpr(patron, Rojo_pull)
v <- unlist(regmatches(Rojo_pull, m))
v
```


```{r}

p <- "\\1_L1 \\2_L2 \\3_R1 \\4_R2"
context <- gsub("([a-z]+) ([a-z]+) ([a-z]+) ([a-z]+)", p, v)
re_match <-  gregexpr(patron, v)
doc_id <- unlist(regmatches(v, re_match))

boton_ner<-boton%>%
  mutate(NER=gregexpr(patron, Letra))%>%
  mutate(Entity=regmatches(Letra,NER))%>%
  unnest(Entity)%>%
  dplyr::select(-c(line:NER))
```



```{r}
corpus2 <- boton_ner %>% 
  group_by(Entity) %>% 
	summarize(doc = paste(Letra, collapse = " "))

corpus2

```


```{r}

dtm <- corpus2 %>% unnest_tokens(input = doc, output = word) %>% 
	count(Entity, word) %>% 
	cast_dtm(document = Entity, term = word, value = n)


mod <- LDA(x = dtm, k = 3, method = "Gibbs", 
          control=list(alpha = 1, seed = 12345, iter = 1000, thin = 1))

```



```{r}
topics <- tidy(mod, matrix="gamma") %>% 
	spread(topic, gamma)

topics%>%
  rename("Acciones"='3',
         "Conectores"="2",
         "Eventos"="1")
```




```{r}
r <- sample.int(n=nrow(corpus2), size=20, replace=FALSE) # Una muestra sin reemplazo
train_dtm <- corpus2[-r, ] %>% unnest_tokens(input=doc, output=word) %>% 
  count(Entity, word) %>% 
  cast_dtm(document=Entity, term=word, value=n)
```

## Modelo

```{r}
train_mod <- LDA(x=train_dtm, k=3, method="Gibbs",
                control=list(alpha=1, seed=10001,
                             iter=1000, thin=1))
```



```{r}
set.seed(12345)
r <- sample.int(n=nrow(corpus2), size=20, replace=FALSE)

## EL vocabulario del escenario de entrenamiento
model_vocab <- tidy(train_mod, matrix="beta") %>% 
  dplyr::select(term) %>% distinct()

model_vocab
```




```{r}
# Tabla idenfiticadoa
test_table <- corpus2[r, ] %>% unnest_tokens(input=doc, output=word) %>% 
  count(Entity, word) %>%
  right_join(model_vocab, by=c("word"="term"))

## Modelo en producción y validación
test_dtm <- test_table %>% 
  arrange(desc(Entity)) %>% 
  mutate(doc_id = ifelse(is.na(Entity), first(Entity), Entity),
         n = ifelse(is.na(n), 0, n)) %>% 
  cast_dtm(document=Entity, term=word, value=n)

test_dtm
```


Probabilidades posteriores -- recuerden a bayes
```{r}
results <- posterior(object=train_mod, newdata=test_dtm)


  # rename("Acciones"='3',
  #        "Conectores"="2",
  #        "Eventos"="1")
results$topics
```



```{r}
model_log<-numeric(20)
model_perp<-numeric(20)
for (i in 2:20){
  modelo<-LDA(train_dtm,k=i,method = 'Gibbs',
              control = list(alpha=0.5,iter=100,seed=1234,thin=1))
  model_log[i]=logLik(modelo)
  model_perp[i]<-perplexity(modelo,test_dtm)
}
```


```{r}
k<-1:20
plot(x=k, y=model_perp, xlab="number of clusters, k", 
     ylab="perplexity score", type="o",xlim = c(1,20))
axis(side = 1, at=1:20)
```

```{r}
mod <- LDA(x = dtm, k = 2, method = "Gibbs", 
          control=list(alpha = 1, seed = 12345, iter = 1000, thin = 1))


topics <- tidy(mod, matrix="gamma") %>% 
	spread(topic, gamma)

topics
```



### LDA 

```{r}
iris%>%
  ggpairs(aes(fill=Species))
```

```{r}
previa<-length(iris$Species[iris$Species=='setosa'])/length(iris$Species)
previa
```


```{r}
par(mfcol = c(3, 4))
for (k in 1:4) {
  j0 <- names(iris)[k]
  x0 <- seq(min(iris[, k]), max(iris[, k]), le = 50)
  for (i in 1:3) {
    i0 <- levels(iris$Species)[i]
    x <- iris[iris$Species == i0, j0]
    hist(x, proba = T, col = grey(0.8), main = paste("especie", i0),
    xlab = j0)
    lines(x0, dnorm(x0, mean(x), sd(x)), col = "red", lwd = 2)
  }
}
```

Prueba de normalidad

```{r}
par(mfcol = c(3, 4))
for (k in 1:4) {
  j0 <- names(iris)[k]
  x0 <- seq(min(iris[, k]), max(iris[, k]), le = 50)
  for (i in 1:3) {
    i0 <- levels(iris$Species)[i]
    x <- iris[iris$Species == i0, j0]
    qqnorm(x, main = paste(i0, j0), pch = 19, col = i + 1) 
    qqline(x)
  }
}
```



```{r}
lda(Species~.,data=iris)
```



```{r}
iris%>%
  ggplot(aes(Sepal.Length,Petal.Width))+
  geom_point(alpha=0.4)
```



```{r}
library(broom)
library(reshape2)
iris_pivoteado<-melt(iris, value.name = "valor")
p_values_iris<-iris_pivoteado%>%
  group_by(Species, variable) %>%
  summarise(p_value_Shapiro.test = round(shapiro.test(valor)$p.value,5))%>%
  mutate(comportamiento=if_else(p_value_Shapiro.test>=0.05,"Normal","Anormal"
))
p_values_iris
```


```{r}
p_values_iris%>%
  mutate(variable=fct_reorder(variable,p_value_Shapiro.test))%>%
  ggplot(aes(p_value_Shapiro.test,variable, color=p_value_Shapiro.test>=0.05))+
  geom_vline(xintercept = 0.05)+
  geom_point()
```


```{r}
data("AssociatedPress")
LDA(AssociatedPress,k = 6,control = list(seed=1224))->lda_noticias
```


```{r}
notcina_topicos_betas<-tidy(lda_noticias,matrix='beta')%>%
  filter(!term %in% stopwords(kind = 'en'))
```



```{r}
notcina_topicos_betas%>%
  group_by(topic)%>%
  top_n(15,beta)%>%
  ungroup()%>%
  arrange(topic,-beta)%>%
  mutate(term=reorder_within(term,beta,topic))%>%
  ggplot(aes(beta,term,fill=factor(topic)))+
  geom_col(show.legend = FALSE)+
  facet_wrap(~topic,scales='free')+
  scale_y_reordered()
  
```



```{r}
tidy(lda_noticias,matrix='gamma')%>%
  arrange(desc(gamma))%>%
  filter(document=='47')
```

